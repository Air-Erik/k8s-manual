# NSX-T Основы для Kubernetes

> **Цель документа:** Объяснить основы NSX-T в контексте развёртывания Kubernetes кластера.
> **Аудитория:** Специалист с базовыми знаниями vSphere, минимальными знаниями NSX-T.
> **Время чтения:** ~10 минут.

---

## Что такое NSX-T и зачем он нужен?

**NSX-T** — это программно-определяемая сеть (SDN) от VMware, которая работает поверх физической инфраструктуры vSphere.

**Простыми словами:**
- Физические коммутаторы и кабели = **Underlay** (базовая инфраструктура)
- NSX-T создаёт **виртуальные сети поверх** физических = **Overlay**
- Виртуальные машины подключаются к **логическим сегментам** NSX-T, а не напрямую к физическим портам

**Зачем это нужно для Kubernetes?**
- Kubernetes нодам (VM) нужна сеть для общения друг с другом
- NSX-T предоставляет эту сеть с **маршрутизацией**, **firewall'ом** и **NAT**
- Мы используем NSX-T как **underlay для VM**, а CNI (Cilium) будет управлять сетью подов

---

## Архитектура NSX-T: Ключевые компоненты

```
┌─────────────────────────────────────────────────────────────┐
│                      Internet / Corp LAN                     │
└───────────────────────────────┬─────────────────────────────┘
                                │
                    ┌───────────▼──────────┐
                    │   Tier-0 Gateway     │  ◄── North-South маршрутизация
                    │   (Physical Router)  │      (выход в интернет/корпсеть)
                    └───────────┬──────────┘
                                │
                    ┌───────────▼──────────┐
                    │   Tier-1 Gateway     │  ◄── East-West + NAT/LB
                    │   (Logical Router)   │      (между сегментами)
                    └───────────┬──────────┘
                                │
        ┌───────────────────────┼───────────────────────┐
        │                       │                       │
  ┌─────▼─────┐         ┌──────▼──────┐        ┌──────▼──────┐
  │ Segment A │         │  Segment B  │        │  Segment C  │
  │ (VIP-VM)  │         │ (k8s-nodes) │        │  (Tanzu)    │
  └─────┬─────┘         └──────┬──────┘        └──────┬──────┘
        │                      │                      │
   ┌────┴────┐            ┌────┴────┐           ┌────┴────┐
   │   VM    │            │   VM    │           │   VM    │
   │  (DB)   │            │ (k8s-w) │           │ (Tanzu) │
   └─────────┘            └─────────┘           └─────────┘
```

### Компоненты, которые тебе нужно понимать:

#### 1. **Transport Zone (TZ)**
- Область видимости для overlay сетей
- Обычно уже настроена при установке NSX-T
- **Для тебя:** Не нужно трогать, просто знай что это граница, внутри которой работают сегменты

#### 2. **Edge Nodes**
- Специальные VM или физические хосты, которые выполняют сервисы (NAT, LB, Firewall для Tier-0/1)
- **Для тебя:** Уже настроены, не трогаем

#### 3. **Transport Nodes**
- ESXi хосты, на которых работают твои VM и overlay сеть NSX-T
- **Для тебя:** Уже настроены, но нужно знать **MTU** (об этом ниже)

---

## Tier-0 Gateway (North-South Routing)

**Что это:**
- Логический роутер, который **подключен к физической сети** (uplink)
- Маршрутизирует трафик **из NSX overlay в интернет/корпсеть** и обратно

**Аналогия:** Это как твой домашний роутер, который подключен к провайдеру.

**Для Kubernetes:**
- Через Tier-0 твои ноды и поды будут ходить в интернет (для apt, docker registry)
- Через Tier-0 пользователи будут заходить на Ingress (если есть публичный IP)

**Что тебе нужно:**
- Убедиться, что **Tier-1** (см. ниже) подключен к Tier-0
- Проверить, есть ли NAT для egress (если IP подсеть внутренняя)

---

## Tier-1 Gateway (East-West + NAT)

**Что это:**
- Логический роутер, который **подключен к Tier-0 сверху** и к **сегментам снизу**
- Обеспечивает:
  - Маршрутизацию между сегментами (East-West)
  - NAT (SNAT для egress, DNAT для ingress)
  - Опционально: Load Balancing (но мы используем MetalLB)

**Аналогия:** Это как коммутатор L3 внутри твоей сети.

**Для Kubernetes:**
- Твой сегмент k8s-nodes будет **подключен к Tier-1**
- Tier-1 даёт **Default Gateway** для VM (например, 192.168.100.1)
- Через Tier-1 трафик идёт в Tier-0 и дальше

**Что тебе нужно:**
- Проверить, что Tier-1 существует и connected к Tier-0
- Проверить, что на Tier-1 есть маршрут по умолчанию (0.0.0.0/0 → Tier-0)

---

## Segments (Логические сети L2)

**Что это:**
- Это **виртуальный VLAN** (логическая L2-сеть)
- VM подключаются к сегменту через vNIC
- Сегмент связан с Tier-1 Gateway (который даёт маршрутизацию)

**Аналогия:** Это как отдельная подсеть/VLAN в физической сети.

**Для Kubernetes:**
- Тебе нужен **один сегмент** для всех k8s-нод (control plane + workers)
- Все ноды будут в **одной L2-сети** (могут пинговать друг друга без роутера)
- Сегмент имеет:
  - **Подсеть** (например, 192.168.100.0/24)
  - **Gateway IP** (например, 192.168.100.1 на Tier-1)
  - **DHCP** (опционально, можно использовать статические IP)

**Пример:**
```
Segment: k8s-nodes-segment
Subnet:  192.168.100.0/24
Gateway: 192.168.100.1 (на Tier-1)
DHCP:    Disabled (или enabled с резервациями)
```

**Важно:**
- Сегмент должен иметь **достаточно IP** для:
  - 3 control plane нод
  - 2+ worker нод (+ запас на рост)
  - 1 API VIP (для kube-vip)
  - 10-20 IP для MetalLB pool

---

## Distributed Firewall (DFW)

**Что это:**
- Firewall, который работает **на каждом ESXi хосте** (не централизованно)
- Фильтрует трафик **между VM** (East-West security)
- Правила применяются **до того**, как пакет покинет хост

**Аналогия:** Это как iptables на каждой VM, но управляется централизованно.

**Для Kubernetes:**
- **КРИТИЧНО:** DFW может **заблокировать** трафик между k8s-нодами!
- Нужно создать правила, которые **разрешают**:
  - Трафик между нодами (для kubelet, API, etcd, CNI)
  - Трафик от пользователей к NodePort/Ingress
  - Egress в интернет (для apt, registry)

**Структура DFW:**
- **Groups** (Группы) — набор VM, IP или тегов (например, группа "k8s-nodes")
- **Rules** (Правила) — Source → Destination + Ports + Action (Allow/Drop)
- **Priority** — правила выполняются **сверху вниз**, первое совпадение применяется

**Пример правила для Kubernetes:**
```
Rule 1 (Priority 1000):
  Source: Group "k8s-nodes"
  Destination: Group "k8s-nodes"
  Ports: Any (или конкретные: 6443, 2379-2380, 10250, 8472)
  Action: Allow

Rule 2 (Priority 1001):
  Source: Any
  Destination: Group "k8s-nodes"
  Ports: 80, 443, 30000-32767
  Action: Allow

Rule 3 (Priority 1002):
  Source: Group "k8s-nodes"
  Destination: Any
  Action: Allow  (egress)
```

**Что тебе нужно:**
- Создать группу для k8s-нод
- Создать правила **ДО** дефолтных deny-правил (с более высоким приоритетом)

---

## SpoofGuard (Защита от подмены IP/MAC)

**Что это:**
- Механизм защиты, который блокирует VM, если она:
  - Отправляет ARP-ответы с IP, который **не принадлежит ей**
  - Отправляет пакеты с source MAC/IP, отличающимися от зарегистрированных

**Аналогия:** Это как "честность" — VM не может притворяться другой VM.

**Зачем это нужно:**
- Защита от ARP spoofing, IP hijacking

**Почему это ПРОБЛЕМА для Kubernetes:**
- **kube-vip** использует **gratuitous ARP**, чтобы анонсировать API VIP на одной из control plane нод
- **MetalLB** (L2 режим) использует **gratuitous ARP**, чтобы анонсировать LoadBalancer IP
- SpoofGuard видит, что VM анонсирует IP, который **не был ей назначен в vCenter**, и **блокирует ARP!**

**Что тебе нужно:**
- Добавить **API VIP** и **MetalLB IP pool** в **whitelist** SpoofGuard
- Или отключить SpoofGuard на портах k8s-нод (менее безопасно, но проще для PoC)

**Как это сделать:**
- NSX UI → Security → SpoofGuard → добавить Allowed IP Addresses для группы k8s-nodes

---

## IPAM/DHCP в NSX-T

**Что это:**
- NSX-T может предоставлять **DHCP** для VM в сегменте
- IPAM (IP Address Management) — пул IP-адресов для автоматического выделения

**Для Kubernetes:**
- **Можно использовать DHCP** для нод (проще в PoC)
- **Или статические IP** (более предсказуемо для Prod)

**Рекомендация:**
- Для PoC: DHCP с **резервациями** (fixed IP по MAC-адресу)
- Для Prod: Статические IP (документированные в табличке)

**Что тебе нужно:**
- Если используешь DHCP: убедись, что **диапазон DHCP** не пересекается с **MetalLB pool** и **API VIP**

---

## MTU в NSX Overlay (КРИТИЧНО!)

**Что это:**
- **MTU** (Maximum Transmission Unit) — максимальный размер пакета
- NSX-T использует **инкапсуляцию** (Geneve) для overlay сетей
- Инкапсуляция добавляет **заголовки** (~50-100 байт)

**Проблема:**
- Если MTU не согласован по цепочке, пакеты фрагментируются или дропаются → **таймауты, медленная работа, потеря соединений**

**Цепочка MTU для Kubernetes:**
```
Physical Network (Underlay)
  MTU: 1600 или 9000 (Jumbo Frames)
       ↓
NSX Transport Node Overlay
  MTU: 1600 (обычно, если underlay 1600)
       ↓
VM vNIC (Node)
  MTU: 1500 (должен быть МЕНЬШЕ overlay на ~100 байт для запаса)
       ↓
Cilium CNI (Pod Network)
  MTU: 1450 (должен быть МЕНЬШЕ VM vNIC на ~50 байт для VXLAN/Geneve)
```

**Что тебе нужно:**
1. Проверить MTU на **NSX Transport Nodes** (NSX UI → System → Fabric → Nodes → Transport Nodes → Overlay MTU)
2. Установить MTU на **VM** = `overlay MTU - 100` (например, 1500 если overlay 1600)
3. Установить MTU в **Cilium** = `VM MTU - 50` (например, 1450 если VM 1500)

**Как проверить:**
```bash
# На ноде проверить MTU интерфейса
ip link show ens192

# Тест с большими пакетами (без фрагментации)
ping -M do -s 1400 <IP-другой-ноды>
```

---

## Краткая шпаргалка: NSX-T для Kubernetes

| Компонент | Что это | Для чего нужно в K8s |
|-----------|---------|---------------------|
| **Tier-0 Gateway** | Роутер в физическую сеть | Egress (интернет), Ingress (публичный доступ) |
| **Tier-1 Gateway** | Роутер для сегментов | Gateway для VM, NAT |
| **Segment** | Логическая L2-сеть | Сеть для k8s-нод (VM) |
| **DFW** | Distributed Firewall | Разрешить трафик между нодами, NodePort, egress |
| **SpoofGuard** | Защита от IP spoofing | Whitelist для VIP и MetalLB |
| **MTU** | Размер пакета | Согласовать end-to-end, иначе проблемы |

---

## Что NSX-T НЕ делает для нашего кластера

**NSX-T НЕ управляет:**
- ❌ Сетью подов (это делает **Cilium CNI**)
- ❌ Service CIDR (это внутри Kubernetes)
- ❌ LoadBalancer IP (это делает **MetalLB**)

**NSX-T только предоставляет:**
- ✅ Сеть для **виртуальных машин** (нод кластера)
- ✅ Маршрутизацию и firewall между нодами и внешним миром
- ✅ Физическую связность (underlay)

**Это называется "NSX as underlay" (в отличие от "NSX as CNI" через NCP).**

---

## Следующие шаги

Теперь, когда ты понял основы NSX-T, перейди к следующему документу:
👉 **[02-tanzu-vs-standalone-k8s.md](./02-tanzu-vs-standalone-k8s.md)**

Там мы разберём, чем отличается Tanzu Supervisor (который у тебя сейчас) от standalone Kubernetes, и почему они могут **сосуществовать** на одном NSX-T.

---

**Вопросы?** Записывай их в `QUESTIONS-FOR-OPERATOR.md` — мы разберём после изучения всех документов! 📝
