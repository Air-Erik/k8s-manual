# Tanzu vs Standalone Kubernetes на NSX-T

> **Цель документа:** Объяснить разницу между Tanzu Supervisor и standalone Kubernetes в контексте NSX-T.
> **Аудитория:** Специалист, работающий в среде с существующим Tanzu.
> **Время чтения:** ~8 минут.

---

## Текущая ситуация: Tanzu уже развёрнут

У тебя в инфраструктуре уже работает **Tanzu Kubernetes Grid (TKG) / Supervisor Cluster**.

**Что это значит:**
- vSphere Supervisor интегрирован с NSX-T через **NSX Container Plugin (NCP)**
- NCP **автоматически** создаёт сети, load balancers, firewall rules для Tanzu workloads
- Это магия, но **привязанная к лицензиям Tanzu**

**Твоя задача:**
- Развернуть **отдельный, независимый** Kubernetes кластер на виртуальных машинах
- **БЕЗ** использования Tanzu/NCP (но на том же NSX-T)

**Хорошие новости:**
- ✅ Tanzu и standalone K8s **могут сосуществовать** на одном NSX-T
- ✅ Они просто используют **разные сегменты** (изолированы друг от друга)
- ✅ Не нужно ничего ломать в Tanzu

---

## Две модели использования NSX-T для Kubernetes

### Модель 1: NSX as CNI (Tanzu с NCP) — у тебя сейчас

```
┌─────────────────────────────────────────────────┐
│            Tanzu Supervisor Cluster             │
│  ┌───────────────────────────────────────────┐  │
│  │         NSX Container Plugin (NCP)        │  │
│  │  ┌─────────────────────────────────────┐  │  │
│  │  │   Pod получает IP из NSX IP Pool    │  │  │
│  │  │   NSX создаёт Segment для NS        │  │  │
│  │  │   NSX создаёт LB для Service        │  │  │
│  │  └─────────────────────────────────────┘  │  │
│  └───────────────────────────────────────────┘  │
└─────────────────────────────────────────────────┘
         ↓
    NSX-T (управляет всем: Pod IP, Service LB, FW)
```

**Характеристики:**
- Pod **напрямую получает IP** из NSX-T (через NCP)
- NSX-T **создаёт отдельный Segment** для каждого Namespace (если настроено)
- Service LoadBalancer **автоматически** создаётся через NSX-T Load Balancer
- NetworkPolicy **реализуется через DFW** в NSX-T
- **CNI = NCP** (встроенный в Tanzu)

**Плюсы:**
- ✅ Глубокая интеграция с NSX-T (единая консоль для сети и security)
- ✅ Автоматизация (меньше ручной работы)
- ✅ Поды видны как объекты в NSX-T

**Минусы:**
- ❌ Требует **лицензию Tanzu**
- ❌ Привязка к vSphere/Tanzu lifecycle
- ❌ Меньше гибкости (не можешь выбрать другой CNI)

---

### Модель 2: NSX as Underlay (наш случай) — что мы делаем

```
┌─────────────────────────────────────────────────┐
│          Standalone Kubernetes Cluster          │
│  ┌───────────────────────────────────────────┐  │
│  │          Cilium CNI (или Calico)          │  │
│  │  ┌─────────────────────────────────────┐  │  │
│  │  │   Pod получает IP из Pod CIDR       │  │  │
│  │  │   Cilium строит VXLAN overlay       │  │  │
│  │  │   MetalLB выделяет LB IP            │  │  │
│  │  └─────────────────────────────────────┘  │  │
│  └───────────────────────────────────────────┘  │
└─────────────────────────────────────────────────┘
         ↓
    VM (Nodes) получают сеть от NSX-T Segment
         ↓
    NSX-T (только underlay: IP для VM, routing, DFW)
```

**Характеристики:**
- **VM-ноды** подключены к NSX-T Segment (получают IP как обычные VM)
- **Поды** получают IP из **Pod CIDR внутри Kubernetes** (10.244.0.0/16)
- CNI (**Cilium**) создаёт **свой overlay** поверх NSX-T overlay (двойная инкапсуляция)
- Service LoadBalancer реализуется через **MetalLB** (софт на нодах)
- NetworkPolicy реализуется **CNI** (Cilium/Calico), а не NSX-T
- **NSX-T не знает о подах** (видит только VM)

**Плюсы:**
- ✅ **Без лицензии Tanzu** (используется kubeadm + Open Source компоненты)
- ✅ Полная гибкость (можешь менять CNI, CSI, LB провайдер)
- ✅ Стандартный Kubernetes (легко мигрировать в другое облако)

**Минусы:**
- ❌ Больше ручной работы (настройка CNI, MetalLB, Ingress)
- ❌ Нет автоматической интеграции с NSX-T (нужно вручную настроить DFW, SpoofGuard)
- ❌ Поды не видны в NSX-T (только ноды-VM)

---

## Сравнение: Tanzu vs Standalone

| Параметр | Tanzu (NSX as CNI) | Standalone (NSX as Underlay) |
|----------|-------------------|------------------------------|
| **Pod Networking** | NSX-T Segment (через NCP) | CNI (Cilium/Calico) |
| **Pod IP** | Из NSX IP Pool | Из Kubernetes Pod CIDR |
| **Service LoadBalancer** | NSX-T Load Balancer | MetalLB (или Avi/NSX ALB) |
| **Ingress** | NSX-T (опционально) | NGINX / Avi VS |
| **NetworkPolicy** | NSX-T DFW | CNI (Cilium/Calico) |
| **Видимость подов в NSX** | Да (как NSX objects) | Нет (только VM) |
| **Лицензия** | Tanzu (платная) | Open Source (бесплатно) |
| **Гибкость** | Низкая (привязка к Tanzu) | Высокая (можно всё менять) |
| **Сложность настройки** | Низкая (автоматизация) | Средняя (ручная настройка) |

---

## Как Tanzu и Standalone K8s сосуществуют?

**Короткий ответ:** Через **разные сегменты** в NSX-T.

### Схема сосуществования:

```
                   NSX-T Tier-1 Gateway
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
  ┌─────▼─────┐      ┌──────▼──────┐    ┌──────▼──────┐
  │ Segment A │      │  Segment B  │    │  Segment C  │
  │ (Tanzu NS)│      │ (k8s-nodes) │    │  (VIP-VM)   │
  └─────┬─────┘      └──────┬──────┘    └──────┬──────┘
        │                   │                   │
  Tanzu Pods          Standalone K8s        Other VMs
  (управляются        (VM-ноды с            (не K8s)
   через NCP)          Cilium)
```

**Изоляция:**
- Tanzu workloads работают в **своих сегментах** (создаваемых NCP)
- Standalone K8s ноды работают в **отдельном сегменте** (например, "k8s-nodes" или "VIP-VM")
- **DFW правила** изолируют трафик (если нужно)

**Общие ресурсы:**
- Tier-0/Tier-1 Gateway (общий роутинг)
- Edge Nodes (общая инфраструктура)
- Transport Zone (общий overlay)

**Конфликты?**
- ❌ **Нет конфликтов IP**, если используются разные подсети
- ❌ **Нет конфликтов сегментов** (разные имена)
- ⚠️ **Возможна конкуренция за ресурсы** (CPU/RAM на ESXi, если хосты общие)

---

## Почему мы НЕ используем NCP?

**NCP (NSX Container Plugin)** — это CNI для Kubernetes, который интегрируется с NSX-T.

**Причины отказа от NCP:**

1. **Лицензия:**
   - NCP требует **лицензию Tanzu**
   - Наша цель — избежать Tanzu-зависимости

2. **Гибкость:**
   - NCP привязывает к vSphere/NSX lifecycle
   - Сложнее мигрировать в другое облако (AWS, Azure)

3. **Избыточность:**
   - Для большинства Kubernetes use-cases достаточно **CNI + MetalLB**
   - NCP добавляет сложность без явных преимуществ (для нашего случая)

4. **Open Source:**
   - Cilium/Calico — активно развиваются, большое сообщество
   - NCP — менее распространён, меньше документации/опыта

**Когда NCP имеет смысл?**
- ✅ Если уже есть Tanzu лицензия и хочется глубокой интеграции
- ✅ Если нужна единая консоль для сети (NSX Manager показывает поды)
- ✅ Если требуется micro-segmentation на уровне подов через DFW

**Наш случай:**
- Нам нужен **независимый кластер** без лицензий
- Cilium даёт **observability** (Hubble), **NetworkPolicy**, **kube-proxy replacement**
- Это современный, гибкий подход

---

## Что нужно знать о существующем Tanzu

**Вопросы, которые нужно выяснить:**

1. **Какие сегменты использует Tanzu?**
   - Нужно узнать, чтобы **не пересекаться** с нашим сегментом
   - Проверить: NSX UI → Networking → Segments → фильтр по тегу "ncp"

2. **Какие IP-диапазоны использует Tanzu?**
   - Pod IP Pool (для подов Tanzu)
   - SNAT IP Pool (для egress)
   - LoadBalancer IP Pool (для Service LoadBalancer)
   - **Убедись, что наш MetalLB pool НЕ пересекается!**

3. **Какие DFW правила настроены для Tanzu?**
   - Проверить: NSX UI → Security → Distributed Firewall
   - Убедись, что наши правила для k8s-nodes **не конфликтуют**

4. **Какой Tier-1 Gateway использует Tanzu?**
   - Можно использовать **тот же** Tier-1 (через разные сегменты)
   - Или создать **новый** Tier-1 (для изоляции)

**Рекомендация:**
- Начать с **отдельного сегмента** (не трогая Tanzu)
- Если существует сегмент "VIP-VM" и он **не используется Tanzu** → можно использовать его
- Если "VIP-VM" связан с Tanzu → создать новый сегмент "k8s-nodes"

---

## Миграция от Tanzu к Standalone (в будущем)

**Это не сейчас**, но полезно понимать стратегию:

### Этап 1: Параллельная работа (сейчас)
- Tanzu Supervisor продолжает работать
- Standalone K8s разворачивается параллельно
- Новые workloads идут в Standalone
- Старые workloads остаются в Tanzu

### Этап 2: Постепенная миграция
- Выбрать не-критичный сервис из Tanzu
- Пересоздать его в Standalone K8s
- Протестировать
- Переключить трафик (через DNS/LB)

### Этап 3: Декомиссия Tanzu (опционально)
- Когда все workloads мигрированы
- Удалить Tanzu Supervisor
- Освободить ресурсы

**Важно:**
- Миграция — это отдельный проект (Этап 3 в плане)
- Сейчас фокус на **запуске** Standalone кластера

---

## Краткая шпаргалка

| Вопрос | Ответ |
|--------|-------|
| **Можно ли использовать NSX-T для Tanzu и Standalone одновременно?** | Да, через разные сегменты |
| **Нужно ли отключать Tanzu?** | Нет, он продолжает работать параллельно |
| **Будут ли конфликты?** | Нет, если IP-диапазоны и сегменты разные |
| **Зачем standalone, если есть Tanzu?** | Избежать лицензий, больше гибкости, стандартный K8s |
| **Почему не NCP?** | Не нужна лицензия Tanzu, хотим Open Source CNI (Cilium) |
| **Что NSX-T делает для standalone K8s?** | Только underlay (сеть для VM, routing, DFW) |
| **Что NSX-T НЕ делает?** | Не управляет подами (это делает Cilium) |

---

## Следующие шаги

Теперь ты понимаешь разницу между Tanzu и standalone Kubernetes на NSX-T. Переходи к следующему документу:
👉 **[03-k8s-network-requirements.md](./03-k8s-network-requirements.md)**

Там мы соберём **чек-лист требований** к NSX-T для нашего кластера — что конкретно нужно настроить.

---

**Вопросы?** Записывай их в `QUESTIONS-FOR-OPERATOR.md`! 📝
